

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>pugh_torch.modules package &mdash; Pugh Torch 0.4.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pugh_torch.optimizers package" href="pugh_torch.optimizers.html" />
    <link rel="prev" title="pugh_torch.models package" href="pugh_torch.models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Pugh Torch
          

          
          </a>

          
            
            
              <div class="version">
                0.4.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#stable-release">Stable release</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#from-sources">From sources</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">Package modules</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="pugh_torch.html">pugh_torch package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="pugh_torch.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.augmentations.html">pugh_torch.augmentations package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.callbacks.html">pugh_torch.callbacks package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.datasets.html">pugh_torch.datasets package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.linalg.html">pugh_torch.linalg package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.losses.html">pugh_torch.losses package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.mappings.html">pugh_torch.mappings package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.metrics.html">pugh_torch.metrics package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.models.html">pugh_torch.models package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">pugh_torch.modules package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.optimizers.html">pugh_torch.optimizers package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.transforms.html">pugh_torch.transforms package</a></li>
<li class="toctree-l4"><a class="reference internal" href="pugh_torch.utils.html">pugh_torch.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pugh_torch.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="pugh_torch.html#module-pugh_torch.exceptions">pugh_torch.exceptions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="pugh_torch.html#module-pugh_torch.helpers">pugh_torch.helpers module</a></li>
<li class="toctree-l3"><a class="reference internal" href="pugh_torch.html#module-pugh_torch">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#deploying">Deploying</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#development">Development</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#the-four-commands-you-need-to-know">The Four Commands You Need To Know</a><ul>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#additional-optional-setup-steps">Additional Optional Setup Steps:</a></li>
<li class="toctree-l3"><a class="reference internal" href="contributing.html#suggested-git-branch-strategy">Suggested Git Branch Strategy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="math.html">Math notation example</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pugh Torch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="modules.html">pugh_torch</a> &raquo;</li>
        
          <li><a href="pugh_torch.html">pugh_torch package</a> &raquo;</li>
        
      <li>pugh_torch.modules package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/pugh_torch.modules.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="pugh_torch.optimizers.html" class="btn btn-neutral float-right" title="pugh_torch.optimizers package" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="pugh_torch.models.html" class="btn btn-neutral float-left" title="pugh_torch.models package" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pugh-torch-modules-package">
<h1>pugh_torch.modules package<a class="headerlink" href="#pugh-torch-modules-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pugh_torch.modules.activation">
<span id="pugh-torch-modules-activation-module"></span><h2>pugh_torch.modules.activation module<a class="headerlink" href="#module-pugh_torch.modules.activation" title="Permalink to this headline">¶</a></h2>
<p>Easy interface for swapping out activation functions, especially those
that may have different weight initialization methods.</p>
<blockquote>
<div><ul class="simple">
<li><p>weights  &lt;- initialization depends on activation function &lt;—-</p></li>
<li><p>normalization                                                 |</p></li>
<li><p>activation &lt;—————————————————</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>To create a new activation, do the following:</dt><dd><ul class="simple">
<li><p>Inherit from ActivationModule to register</p></li>
<li><p>[optional] implement <code class="docutils literal notranslate"><span class="pre">init_layer</span></code> method</p></li>
<li><p>[optional] implement <code class="docutils literal notranslate"><span class="pre">init_first_layer</span></code> method</p></li>
</ul>
</dd>
<dt>One this is done, your activation function will me available as:</dt><dd><p>Activation(“myactivationlowercase”, <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs)</p>
</dd>
</dl>
<dl class="function">
<dt id="pugh_torch.modules.activation.Activation">
<code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Activation</code><span class="sig-paren">(</span><em class="sig-param">name</em>, <em class="sig-param">init_layers=None</em>, <em class="sig-param">*</em>, <em class="sig-param">first=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Activation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Activation" title="Permalink to this definition">¶</a></dt>
<dd><p>Activation Factory Function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – Activation function type</p></li>
<li><p><strong>init_layers</strong> (<em>nn.Module or list of nn.Module</em>) – Weights that need initialization based on</p></li>
<li><p><strong>kwargs</strong> (<em>dict</em>) – Passed along to activation function constructor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.ActivationModule">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">ActivationModule</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ActivationModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ActivationModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Only used to automatically register activation functions.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.ActivationModule.init_first_layer">
<code class="sig-name descname">init_first_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ActivationModule.init_first_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ActivationModule.init_first_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.activation.ActivationModule.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ActivationModule.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ActivationModule.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.ActivationModule.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.ActivationModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.CELU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">CELU</code><span class="sig-paren">(</span><em class="sig-param">alpha: float = 1.0</em>, <em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#CELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.CELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.CELU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="pugh_torch.modules.activation.CELU.alpha">
<code class="sig-name descname">alpha</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.CELU.alpha" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.activation.CELU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#CELU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.CELU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.CELU.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.CELU.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.ELU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">ELU</code><span class="sig-paren">(</span><em class="sig-param">alpha: float = 1.0</em>, <em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.ELU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="pugh_torch.modules.activation.ELU.alpha">
<code class="sig-name descname">alpha</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.ELU.alpha" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.activation.ELU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ELU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ELU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.ELU.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.ELU.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.GELU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">GELU</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#GELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.GELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.GELU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.GELU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#GELU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.GELU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.GELU.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.GELU.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Hardshrink">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Hardshrink</code><span class="sig-paren">(</span><em class="sig-param">lambd: float = 0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Hardshrink</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Hardshrink.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardshrink.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardshrink.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Hardshrink.lambd">
<code class="sig-name descname">lambd</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Hardshrink.lambd" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Hardsigmoid">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Hardsigmoid</code><span class="sig-paren">(</span><em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardsigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardsigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Hardsigmoid</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Hardsigmoid.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardsigmoid.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardsigmoid.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Hardsigmoid.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Hardsigmoid.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Hardswish">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Hardswish</code><span class="sig-paren">(</span><em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardswish"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardswish" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Hardswish</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Hardswish.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardswish.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardswish.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Hardswish.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Hardswish.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Hardtanh">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Hardtanh</code><span class="sig-paren">(</span><em class="sig-param">min_val: float = -1.0</em>, <em class="sig-param">max_val: float = 1.0</em>, <em class="sig-param">inplace: bool = False</em>, <em class="sig-param">min_value: Optional[float] = None</em>, <em class="sig-param">max_value: Optional[float] = None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardtanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardtanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Hardtanh</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Hardtanh.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Hardtanh.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Hardtanh.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Hardtanh.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Hardtanh.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Hardtanh.max_val">
<code class="sig-name descname">max_val</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Hardtanh.max_val" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Hardtanh.min_val">
<code class="sig-name descname">min_val</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Hardtanh.min_val" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.LeakyReLU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">LeakyReLU</code><span class="sig-paren">(</span><em class="sig-param">negative_slope: float = 0.01</em>, <em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#LeakyReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.LeakyReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.LeakyReLU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.LeakyReLU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#LeakyReLU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.LeakyReLU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.LeakyReLU.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.LeakyReLU.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.LeakyReLU.negative_slope">
<code class="sig-name descname">negative_slope</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.LeakyReLU.negative_slope" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.LogSigmoid">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">LogSigmoid</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#LogSigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.LogSigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.LogSigmoid</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.LogSigmoid.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#LogSigmoid.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.LogSigmoid.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.LogSigmoid.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.LogSigmoid.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.MultiheadAttention">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">MultiheadAttention</code><span class="sig-paren">(</span><em class="sig-param">embed_dim</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">dropout=0.0</em>, <em class="sig-param">bias=True</em>, <em class="sig-param">add_bias_kv=False</em>, <em class="sig-param">add_zero_attn=False</em>, <em class="sig-param">kdim=None</em>, <em class="sig-param">vdim=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#MultiheadAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.MultiheadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.MultiheadAttention</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="pugh_torch.modules.activation.MultiheadAttention.bias_k">
<code class="sig-name descname">bias_k</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.MultiheadAttention.bias_k" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.MultiheadAttention.bias_v">
<code class="sig-name descname">bias_v</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.MultiheadAttention.bias_v" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Noop">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Noop</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Noop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Noop" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Noop.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Noop.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Noop.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Noop.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Noop.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.PReLU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">PReLU</code><span class="sig-paren">(</span><em class="sig-param">num_parameters: int = 1</em>, <em class="sig-param">init: float = 0.25</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#PReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.PReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.PReLU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.PReLU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#PReLU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.PReLU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.PReLU.num_parameters">
<code class="sig-name descname">num_parameters</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.PReLU.num_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.RReLU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">RReLU</code><span class="sig-paren">(</span><em class="sig-param">lower: float = 0.125</em>, <em class="sig-param">upper: float = 0.3333333333333333</em>, <em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#RReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.RReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.RReLU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.RReLU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#RReLU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.RReLU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.RReLU.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.RReLU.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.RReLU.lower">
<code class="sig-name descname">lower</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.RReLU.lower" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.RReLU.upper">
<code class="sig-name descname">upper</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.RReLU.upper" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.ReLU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">ReLU</code><span class="sig-paren">(</span><em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.ReLU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.ReLU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ReLU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ReLU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.ReLU.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.ReLU.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.ReLU6">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">ReLU6</code><span class="sig-paren">(</span><em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ReLU6"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ReLU6" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.ReLU6</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.ReLU6.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#ReLU6.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.ReLU6.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.ReLU6.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.ReLU6.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.ReLU6.max_val">
<code class="sig-name descname">max_val</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.ReLU6.max_val" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.ReLU6.min_val">
<code class="sig-name descname">min_val</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.ReLU6.min_val" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.SELU">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">SELU</code><span class="sig-paren">(</span><em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#SELU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.SELU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.SELU</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.SELU.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#SELU.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.SELU.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.SELU.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.SELU.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Sigmoid">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Sigmoid</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Sigmoid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Sigmoid" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Sigmoid</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Sigmoid.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Sigmoid.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Sigmoid.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Sigmoid.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Sigmoid.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Sine">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Sine</code><span class="sig-paren">(</span><em class="sig-param">frequency=30</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Sine"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Sine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Implicit Neural Representations with Periodic Activation Functions
<a class="reference external" href="https://arxiv.org/pdf/2006.09661.pdf">https://arxiv.org/pdf/2006.09661.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Sine.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Sine.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Sine.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.activation.Sine.init_first_layer">
<code class="sig-name descname">init_first_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Sine.init_first_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Sine.init_first_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.activation.Sine.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Sine.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Sine.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Sine.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Sine.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Softplus">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Softplus</code><span class="sig-paren">(</span><em class="sig-param">beta: int = 1</em>, <em class="sig-param">threshold: int = 20</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Softplus"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Softplus" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Softplus</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="pugh_torch.modules.activation.Softplus.beta">
<code class="sig-name descname">beta</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Softplus.beta" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.activation.Softplus.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Softplus.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Softplus.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Softplus.threshold">
<code class="sig-name descname">threshold</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Softplus.threshold" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Softshrink">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Softshrink</code><span class="sig-paren">(</span><em class="sig-param">lambd: float = 0.5</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Softshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Softshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Softshrink</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Softshrink.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Softshrink.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Softshrink.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Softshrink.lambd">
<code class="sig-name descname">lambd</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Softshrink.lambd" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Softsign">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Softsign</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Softsign"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Softsign" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Softsign</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Softsign.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Softsign.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Softsign.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Softsign.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Softsign.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Tanh">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Tanh</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Tanh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Tanh" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Tanh</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Tanh.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Tanh.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Tanh.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Tanh.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Tanh.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Tanhshrink">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Tanhshrink</code><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Tanhshrink"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Tanhshrink" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Tanhshrink</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.activation.Tanhshrink.init_layer">
<code class="sig-name descname">init_layer</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Tanhshrink.init_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Tanhshrink.init_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this in child activation function</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Tanhshrink.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Tanhshrink.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.activation.Threshold">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.activation.</code><code class="sig-name descname">Threshold</code><span class="sig-paren">(</span><em class="sig-param">threshold: float</em>, <em class="sig-param">value: float</em>, <em class="sig-param">inplace: bool = False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/activation.html#Threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.activation.Threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.activation.Threshold</span></code>, <a class="reference internal" href="#pugh_torch.modules.activation.ActivationModule" title="pugh_torch.modules.activation.ActivationModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.activation.ActivationModule</span></code></a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="attribute">
<dt id="pugh_torch.modules.activation.Threshold.inplace">
<code class="sig-name descname">inplace</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Threshold.inplace" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Threshold.threshold">
<code class="sig-name descname">threshold</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Threshold.threshold" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.activation.Threshold.value">
<code class="sig-name descname">value</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.activation.Threshold.value" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pugh_torch.modules.conv">
<span id="pugh-torch-modules-conv-module"></span><h2>pugh_torch.modules.conv module<a class="headerlink" href="#module-pugh_torch.modules.conv" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="pugh_torch.modules.conv.conv1x1">
<code class="sig-prename descclassname">pugh_torch.modules.conv.</code><code class="sig-name descname">conv1x1</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">out_planes</em>, <em class="sig-param">stride=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/conv.html#conv1x1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.conv.conv1x1" title="Permalink to this definition">¶</a></dt>
<dd><p>1x1 convolution</p>
</dd></dl>

<dl class="function">
<dt id="pugh_torch.modules.conv.conv3x3">
<code class="sig-prename descclassname">pugh_torch.modules.conv.</code><code class="sig-name descname">conv3x3</code><span class="sig-paren">(</span><em class="sig-param">in_planes</em>, <em class="sig-param">out_planes</em>, <em class="sig-param">stride=1</em>, <em class="sig-param">groups=1</em>, <em class="sig-param">dilation=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/conv.html#conv3x3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.conv.conv3x3" title="Permalink to this definition">¶</a></dt>
<dd><p>3x3 convolution with padding</p>
</dd></dl>

</div>
<div class="section" id="module-pugh_torch.modules.hash">
<span id="pugh-torch-modules-hash-module"></span><h2>pugh_torch.modules.hash module<a class="headerlink" href="#module-pugh_torch.modules.hash" title="Permalink to this headline">¶</a></h2>
<p>Right now, only <code class="docutils literal notranslate"><span class="pre">RandHashProj</span></code> is recommended for use.</p>
<dl class="simple">
<dt>PyTorch Hashing code is based on code from:</dt><dd><p><a class="reference external" href="https://github.com/ma3oun/hrn">https://github.com/ma3oun/hrn</a></p>
</dd>
</dl>
<p>Values are stored as gradient-less parameters so they get properly saved.</p>
<dl class="class">
<dt id="pugh_torch.modules.hash.BinaryMHash">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.hash.</code><code class="sig-name descname">BinaryMHash</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#BinaryMHash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.BinaryMHash" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pugh_torch.modules.hash.MHash" title="pugh_torch.modules.hash.MHash"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.hash.MHash</span></code></a></p>
<p>Special case of MHash where the output is in the set {-1, 1}</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>p</strong> (<em>int</em>) – Large prime number, larger than the size of the universe input
set. The default value is a random large prime number that should be sufficient for most use-cases.</p>
</dd>
</dl>
<dl class="method">
<dt id="pugh_torch.modules.hash.BinaryMHash.hash">
<code class="sig-name descname">hash</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#BinaryMHash.hash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.BinaryMHash.hash" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.hash.BinaryMHash.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.hash.BinaryMHash.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.hash.Hash">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.hash.</code><code class="sig-name descname">Hash</code><span class="sig-paren">(</span><em class="sig-param">m</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#Hash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.Hash" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Base module for other pytorch hash functions.</p>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>torch.Tensor</em>) – Scalar output dimensionality (output hash size)</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><p><strong>m</strong> (<em>int</em>) – Output size of this hash function</p>
</dd>
</dl>
<dl class="method">
<dt id="pugh_torch.modules.hash.Hash.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#Hash.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.Hash.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Tensor of any shape; this hash function will be applied element-wise.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.hash.Hash.hash">
<code class="sig-name descname">hash</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#Hash.hash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.Hash.hash" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.hash.Hash.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.hash.Hash.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.hash.MHash">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.hash.</code><code class="sig-name descname">MHash</code><span class="sig-paren">(</span><em class="sig-param">m</em>, <em class="sig-param">p=None</em>, <em class="sig-param">a=None</em>, <em class="sig-param">b=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#MHash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.MHash" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pugh_torch.modules.hash.Hash" title="pugh_torch.modules.hash.Hash"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.hash.Hash</span></code></a></p>
<p>Multiplicative Universal Hashing</p>
<p>First described by Lawrence Carter and Mark Wegman
Universal Hash Function</p>
<dl class="simple">
<dt>See:</dt><dd><p><a class="reference external" href="https://jeffe.cs.illinois.edu/teaching/datastructures/notes/12-hashing.pdf">https://jeffe.cs.illinois.edu/teaching/datastructures/notes/12-hashing.pdf</a></p>
</dd>
</dl>
<p>output = ((a * input + b) % p) % m</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>m</strong> (<em>int</em>) – Size of output hash.</p></li>
<li><p><strong>p</strong> (<em>int</em>) – Large prime number, larger than the size of the universe input
set. Defaults to a random prime at least 10x bigger than <code class="docutils literal notranslate"><span class="pre">m</span></code>.</p></li>
<li><p><strong>a</strong> (<em>int</em>) – Salt A. If not explicitly set, randomly initialized.</p></li>
<li><p><strong>b</strong> (<em>int</em>) – Salt B. If not explicitly set, randomly initialized.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pugh_torch.modules.hash.MHash.from_offset">
<em class="property">classmethod </em><code class="sig-name descname">from_offset</code><span class="sig-paren">(</span><em class="sig-param">m</em>, <em class="sig-param">p</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#MHash.from_offset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.MHash.from_offset" title="Permalink to this definition">¶</a></dt>
<dd><p>Set prime number via index into a list of primes starting from 3.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>p</strong> (<em>int</em>) – Index into list of primes to use.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.hash.MHash.hash">
<code class="sig-name descname">hash</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#MHash.hash"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.MHash.hash" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.hash.MHash.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.hash.MHash.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.hash.MHashProj">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.hash.</code><code class="sig-name descname">MHashProj</code><span class="sig-paren">(</span><em class="sig-param">out_feat</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#MHashProj"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.MHashProj" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.container.ParameterDict</span></code></p>
<p>Hashes and projects and arbitrary-feature-length input into a
fixed-feature-length output.</p>
<p>Applies a random feature hashing function.</p>
<dl class="simple">
<dt>This is the function PHI described in section 2 of:</dt><dd><p><a class="reference external" href="https://arxiv.org/abs/2010.05880">https://arxiv.org/abs/2010.05880</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>out_features</strong> (<em>int</em>) – The output hash embedding size</p>
</dd>
</dl>
<dl class="method">
<dt id="pugh_torch.modules.hash.MHashProj.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#MHashProj.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.MHashProj.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – (b, input_feat) Tensor to hash</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(b, output_feat) Hashed tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.hash.MHashProj.from_hashers">
<em class="property">classmethod </em><code class="sig-name descname">from_hashers</code><span class="sig-paren">(</span><em class="sig-param">hash_h</em>, <em class="sig-param">hash_xi</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#MHashProj.from_hashers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.MHashProj.from_hashers" title="Permalink to this definition">¶</a></dt>
<dd><p>More advanced initialization from externally defined hashers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hash_h</strong> (<em>pugh_torch.modules.Hash</em>) – Hashing function that outputs in set <code class="docutils literal notranslate"><span class="pre">{0,</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">out_feat-1}</span></code></p></li>
<li><p><strong>hash_xi</strong> (<em>pugh_torch.modules.Hash</em>) – Binary hashing function that outputs in set <code class="docutils literal notranslate"><span class="pre">{-1,</span> <span class="pre">1}</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.hash.MHashProj.prime_offset">
<code class="sig-name descname">prime_offset</code><em class="property"> = 2000</em><a class="headerlink" href="#pugh_torch.modules.hash.MHashProj.prime_offset" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.hash.MHashProj.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#MHashProj.to"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.MHashProj.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Records the device to <code class="docutils literal notranslate"><span class="pre">self.device</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.hash.MHashProj.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.hash.MHashProj.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="pugh_torch.modules.hash.RandHashProj">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.hash.</code><code class="sig-name descname">RandHashProj</code><span class="sig-paren">(</span><em class="sig-param">out_feat</em>, <em class="sig-param">sparse=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#RandHashProj"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.RandHashProj" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>We can just extend a single projection matrix without the
need for two separate hash functions.</p>
<p>This algorithm deterministically maps an arbitrarily long <code class="docutils literal notranslate"><span class="pre">in_feat</span></code>
vector into a fixed-length <code class="docutils literal notranslate"><span class="pre">out_feat</span></code> vector. It accomplishes this by
the following algorithm:</p>
<blockquote>
<div><dl class="simple">
<dt>For each element in the input feature vector:</dt><dd><ol class="arabic simple">
<li><p>Based on the index, deterministically multiply it by <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">-1</span></code></p></li>
<li><p>Based on the index, deterministically map it to a single element
in the output feature vector.</p></li>
</ol>
</dd>
</dl>
<p>Each element in the output feature vector is the sum of all the
input elements mapped to it.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Variables</dt>
<dd class="field-odd"><p><strong>proj</strong> (<em>torch.nn.Parameter</em>) – (out_feat, in_feat) where in_feat is the maximum input feature
size fed through yet.</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>out_feat</strong> (<em>int</em>) – Output feature size</p></li>
<li><p><strong>sparse</strong> (<em>bool</em>) – Use a sparse representation for the internal projection matrix.
Saves a good amount of memory when <code class="docutils literal notranslate"><span class="pre">out_feat&gt;5</span></code>, which is a pretty
typical use-case.
Defaults to whichever representation would be more memory efficient.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pugh_torch.modules.hash.RandHashProj.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#RandHashProj.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.RandHashProj.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – (B, N) feature vector</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="pugh_torch.modules.hash.RandHashProj.sparse">
<em class="property">property </em><code class="sig-name descname">sparse</code><a class="headerlink" href="#pugh_torch.modules.hash.RandHashProj.sparse" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.hash.RandHashProj.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.hash.RandHashProj.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="pugh_torch.modules.hash.primes">
<code class="sig-prename descclassname">pugh_torch.modules.hash.</code><code class="sig-name descname">primes</code><span class="sig-paren">(</span><em class="sig-param">n</em>, <em class="sig-param">copy=False</em>, <em class="sig-param">cache=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#primes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.primes" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a array of primes, 3 &lt;= p &lt; n</p>
<dl class="simple">
<dt>This is very fast, the following takes &lt;1 second:</dt><dd><p>res = primes(100_000_000)
assert len(res) == 5_761_454
assert res[0] == 3
assert res[-1] == 99_999_989</p>
</dd>
</dl>
<p>Caches the largest <code class="docutils literal notranslate"><span class="pre">n</span></code> array for future calls.</p>
<dl class="simple">
<dt>Modified from:</dt><dd><p><a class="reference external" href="https://stackoverflow.com/a/3035188/13376237">https://stackoverflow.com/a/3035188/13376237</a></p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> (<em>int</em>) – Generate primes up to this number</p></li>
<li><p><strong>copy</strong> (<em>bool</em>) – Copy the output array from the internal cache. Only set to <code class="docutils literal notranslate"><span class="pre">true</span></code>
if you intend to modify the returned array inplace.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>cache</strong> (<em>bool</em>) – Use the internal cache for generating/storing prime values.
Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Array of output primes. Do not modify this array inplace unless
you set <code class="docutils literal notranslate"><span class="pre">copy=True</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="pugh_torch.modules.hash.primes_index">
<code class="sig-prename descclassname">pugh_torch.modules.hash.</code><code class="sig-name descname">primes_index</code><span class="sig-paren">(</span><em class="sig-param">i</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/hash.html#primes_index"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.hash.primes_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the prime value at index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<em>int</em>) – Index into the list of primes (starting at 3) to get.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-pugh_torch.modules.init">
<span id="pugh-torch-modules-init-module"></span><h2>pugh_torch.modules.init module<a class="headerlink" href="#module-pugh_torch.modules.init" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>Equivalent names:</dt><dd><ul class="simple">
<li><p>he == kaming</p></li>
<li><p>xavier == glorot</p></li>
</ul>
</dd>
<dt>Rules of thumb collected from various sources:</dt><dd><ul class="simple">
<li><p>Use He for ReLU</p></li>
<li><p>Use xavier for tanh</p></li>
</ul>
</dd>
</dl>
<dl class="function">
<dt id="pugh_torch.modules.init.he">
<code class="sig-prename descclassname">pugh_torch.modules.init.</code><code class="sig-name descname">he</code><span class="sig-paren">(</span><em class="sig-param">m</em>, <em class="sig-param">mode='fan_in'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/init.html#he"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.init.he" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="function">
<dt id="pugh_torch.modules.init.xavier">
<code class="sig-prename descclassname">pugh_torch.modules.init.</code><code class="sig-name descname">xavier</code><span class="sig-paren">(</span><em class="sig-param">m</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/init.html#xavier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.init.xavier" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-pugh_torch.modules.lightning_module">
<span id="pugh-torch-modules-lightning-module-module"></span><h2>pugh_torch.modules.lightning_module module<a class="headerlink" href="#module-pugh_torch.modules.lightning_module" title="Permalink to this headline">¶</a></h2>
<p>Extends pytorch-lightning’s LightningModule for some
quality of life improvements.</p>
<dl class="class">
<dt id="pugh_torch.modules.lightning_module.LightningModule">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.lightning_module.</code><code class="sig-name descname">LightningModule</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/lightning_module.html#LightningModule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.lightning_module.LightningModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pugh_torch.modules.load_state_dict_mixin.LoadStateDictMixin" title="pugh_torch.modules.load_state_dict_mixin.LoadStateDictMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">pugh_torch.modules.load_state_dict_mixin.LoadStateDictMixin</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code></p>
<dl class="method">
<dt id="pugh_torch.modules.lightning_module.LightningModule.configure_optimizers">
<code class="sig-name descname">configure_optimizers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/lightning_module.html#LightningModule.configure_optimizers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.lightning_module.LightningModule.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Pretty good defaults, can be easily overrided</p>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.lightning_module.LightningModule.training">
<code class="sig-name descname">training</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.lightning_module.LightningModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pugh_torch.modules.load_state_dict_mixin">
<span id="pugh-torch-modules-load-state-dict-mixin-module"></span><h2>pugh_torch.modules.load_state_dict_mixin module<a class="headerlink" href="#module-pugh_torch.modules.load_state_dict_mixin" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pugh_torch.modules.load_state_dict_mixin.LoadStateDictMixin">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.load_state_dict_mixin.</code><code class="sig-name descname">LoadStateDictMixin</code><a class="reference internal" href="_modules/pugh_torch/modules/load_state_dict_mixin.html#LoadStateDictMixin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.load_state_dict_mixin.LoadStateDictMixin" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="pugh_torch.modules.load_state_dict_mixin.LoadStateDictMixin.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em>, <em class="sig-param">strict=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/load_state_dict_mixin.html#LoadStateDictMixin.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.load_state_dict_mixin.LoadStateDictMixin.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Confirms and logs the weights that you expect are loaded when
<code class="docutils literal notranslate"><span class="pre">strict=False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>changed</strong> (<em>list of str</em>) – Parameter names that were updated via loading the state_dict</p></li>
<li><p><strong>unchanged</strong> (<em>list of str</em>) – Parameter names that were NOT updated via loading the state_dict</p></li>
<li><p><strong>shape_mismatch</strong> (<em>list of str</em>) – Parameter names that were NOT updated via loading the state_dict
due to the model’s paramemeter shape not matching the state_dict.
This is strictly a subset of <code class="docutils literal notranslate"><span class="pre">unchanged</span></code>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pugh_torch.modules.meta">
<span id="pugh-torch-modules-meta-module"></span><h2>pugh_torch.modules.meta module<a class="headerlink" href="#module-pugh_torch.modules.meta" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pugh_torch.modules.meta.BatchLinear">
<em class="property">class </em><code class="sig-prename descclassname">pugh_torch.modules.meta.</code><code class="sig-name descname">BatchLinear</code><span class="sig-paren">(</span><em class="sig-param">in_features: int</em>, <em class="sig-param">out_features: int</em>, <em class="sig-param">bias: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/meta.html#BatchLinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.meta.BatchLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.linear.Linear</span></code></p>
<p>Linear layer that can take batched weights and bias at runtime.</p>
<p>Technically a little wasteful because we might be allocating
some parameters that aren’t used, but its usually a very small amount of
memory.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="method">
<dt id="pugh_torch.modules.meta.BatchLinear.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">weight=None</em>, <em class="sig-param">bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/pugh_torch/modules/meta.html#BatchLinear.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pugh_torch.modules.meta.BatchLinear.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – (B, <a href="#id3"><span class="problematic" id="id4">*</span></a>, feat_in) Some input tensor</p></li>
<li><p><strong>weight</strong> (<em>torch.Tensor</em>) – (B feat_out, feat_in)  If provided, doesn’t use internal weights</p></li>
<li><p><strong>bias</strong> – (B, feat_out) If provided, doesn’t use internal bias</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.meta.BatchLinear.in_features">
<code class="sig-name descname">in_features</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.meta.BatchLinear.in_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.meta.BatchLinear.out_features">
<code class="sig-name descname">out_features</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.meta.BatchLinear.out_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="pugh_torch.modules.meta.BatchLinear.weight">
<code class="sig-name descname">weight</code><em class="property"> = None</em><a class="headerlink" href="#pugh_torch.modules.meta.BatchLinear.weight" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pugh_torch.modules">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pugh_torch.modules" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Brian Pugh

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>